{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 빅데이터프로그래밍\n",
    "\n",
    "       \n",
    "4주 1강: String, Regular Expressions, and Languages (2)\n",
    "<br/><br/>\n",
    "\n",
    "숭실대학교<br/>\n",
    "AI융합학부<br/>\n",
    "윤진혁<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 오늘 배울 것들\n",
    "\n",
    "* 그 이후 정규표현식 (Regular Expressions) 을 통해서 텍스트를 다루는 법을 공부합니다.\n",
    "* python의 자연어 처리기인 nltk를 통해서 텍스트 데이터를 다루는 법을 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 정규표현식과 re 모듈을 배워봅시다\n",
    "우리는 위의 예들에서 단순한 규칙을 가진 텍스트를 다루는 법을 배웠습니다.\n",
    "그런데 조금 더 복잡한 규칙을 가진 텍스트 데이터를 처리하려면 어떻게 해야할까요? 예를 들어서 B부터 D까지의 문자를 모두 A라는 문자로 치환하고 싶을 때는 어떻게 해야할까요?\n",
    "\n",
    "이런 경우를 위해서 파이썬에는 \"정규표현식 (Regular expression)\"을 다루는 모듈인 re가 있습니다.예를 들어 노랑색 강조 부분은 다음 정규식을 사용했을 때 매치되는 부분을 표시해 둔 것입니다  ```(?<=\\.) {2,}(?=[A-Z])```\n",
    "<img src=\"./Figs/The_river_effect_in_justified_text.jpg\" width = 240>\n",
    "위의 식을 해석하면 ```.```과 영문대문자 사이의 최소 2개 이상의 공백문자를 매칭하라는 뜻입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(?<=\\.) {2,}(?=[A-Z])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ". 이라는 문자와 영문 대문자 A-Z 사이에 공백 이 최소 2개 이상 인 경우 매칭해라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "re모듈을 불러와 봅시다. 기본 모듈이니 별도로 설치할 필요는 없습니다. \n",
    "\n",
    "먼저 re.search와 re.match는는 문장 안에 정규표현식과 매칭되는 단어가 있는지 찾아줍니다.\n",
    "\n",
    "match는 맨 처음부터 일치하는 경우만 찾아주고, search는 중간부터도 찾아줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re #re 모듈 불러오기 \n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my Life\"\n",
    "\n",
    "#match는 맨 처음부터 일치하는 경우만 찾아줍니다\n",
    "print(re.match('Life', txt1)) #Life로 시작하는 경우를 찾아줌.\n",
    "print(re.match('Life', txt2))\n",
    "print(re.match('life', txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='Life'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my Life\"\n",
    "\n",
    "#search는 어디에 있든지 찾아줍니다\n",
    "print(re.search('Life', txt1)) #위치가 어디에 있는지 찾아줌. span - 위치를 인덱스 값으로 리턴\n",
    "print(re.search('Life', txt2)) \n",
    "print(re.search('life', txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life\n",
      "0\n",
      "4\n",
      "(0, 4)\n"
     ]
    }
   ],
   "source": [
    "match = re.search('Life', txt1)\n",
    "# group 함수를 통해서 매칭된 값을 얻을 수 있습니다.\n",
    "print(match.group()) #매칠된 값 Life가 리턴됨\n",
    "\n",
    "# start, end, span 함수를 통해서 텍스트의 어느 부분에서 매칭이 일어났는지 알 수 있습니다.\n",
    "print(match.start()) #어느 부분에서 매칭이 일어났는지가 반롼됨\n",
    "print(match.end())\n",
    "print(match.span()) # span은 어디서부터, 어디까지인지 2개의 값이 튜플로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='life'>\n",
      "Life\n",
      "life\n"
     ]
    }
   ],
   "source": [
    "# life와 Life를 모두 찾고 싶다면 어떻게 할까요?\n",
    "# 첫번째 방법은 life Life를 모두 검색하는 것입니다\n",
    "# | 은 or 연산자와 같습니다. 앞과 뒤의 것 중 하나라도 맞으면 됩니다.\n",
    "# 이를 \"선택 (selection)\"이라고 합니다. \n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my life\"\n",
    "\n",
    "print(re.search(\"Life|life\", txt1)) #셀렉션 문자 |를 사용해서 or로 문자열 search 가능\n",
    "print(re.search(\"Life|life\", txt2))\n",
    "\n",
    "print(re.search(\"Life|life\", txt1).group()) # 대문자와 매칭된 결과가 나온다.\n",
    "print(re.search(\"Life|life\", txt2).group()) #소문자와 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='life'>\n",
      "Life\n",
      "life\n"
     ]
    }
   ],
   "source": [
    "# 두번째 방법은 Life의 맨 앞글자를 l혹은 L으로 검색하라고 하는 것입니다.\n",
    "# [] 대괄호로 묶어주면 됩니다. 이를 \"문자 클래스 chracter class\"라고 합니다\n",
    "\n",
    "print(re.search(\"[Ll]ife\", txt1)) # 대문자 L과 소문자 l을 [] 대괄호로 묶어주면 한번에 적기 가능\n",
    "print(re.search(\"[Ll]ife\", txt2))\n",
    "\n",
    "print(re.search(\"[Ll]ife\", txt1).group())\n",
    "print(re.search(\"[Ll]ife\", txt2).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(14, 18), match='life'>\n"
     ]
    }
   ],
   "source": [
    "# ^은 문장의 가장 처음을 의미합니다. $은 문장의 가장 마지막을 의미합니다.\n",
    "# 아래와 같이 하면 문장의 맨 처음이 매칭되거나 맨 마지막에 매칭되게 할 수 있습니다. \n",
    "# ^와 $를 위치를 이야기한다는 뜻에서 \"앵커 (anchor)라고 합니다\"\n",
    "\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my life\"\n",
    "\n",
    "print(re.search(\"^[Ll]ife\", txt1)) #문장의 맨 처음에 매칭되는 조건을 걸어 검색하려면\n",
    "print(re.search(\"^[Ll]ife\", txt2)) #함수를 바꾸는 것 보다 정규표현식에 ^를 붙여 수정하는게 편함이득\n",
    "\n",
    "print(re.search(\"[Ll]ife$\", txt1)) # 끝에 매칭되는 조건을 걸고싶다면 마지막에 $를 붙여주기\n",
    "print(re.search(\"[Ll]ife$\", txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(16, 24), match=' life   '>\n"
     ]
    }
   ],
   "source": [
    "# 앞 뒤로 공백이 있다면 어떻게 할까요? \\s는 공백문자열을 의미합니다 .\n",
    "# \\ 는 이스케이프라고 합니다. 표시 불가능한 문자(공백, 개행, 탭 등)를 표현하는데 쓰입니다.\n",
    "txt2 = \"   My baby is my life   \"\n",
    "\n",
    "# 아래처럼 하면 검색이 안됩니다.\n",
    "print(re.search(\"[Ll]ife$\", txt2)) #본래 문장에 앞뒤로 공백이 있어서 검색 불가\n",
    "\n",
    "# \\s*은 공백문자열을 몇개라도 포함하게 해도 괜찮다는 뜻입니다.\n",
    "# *은 수량자라고 부릅니다. 다른 수량자들도 있습니다.\n",
    "print(re.search(\"\\s*[Ll]ife\\s*$\", txt2)) # \\s를 적어서 앞 뒤에 공백이 있어도 포함하라고 지시\n",
    "#공백이 몇개인지 모르므로 *(애스터리스크)를 붙여서 조건을 걸어준다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "match와 search는 가장 처음 등장한 한 번만 찾아줍니다.\n",
    "\n",
    "등장하는 경우를 모두 찾고 싶으면 findall을 씁니다.\n",
    "\n",
    "다만 이 경우는 리스트로 리턴해 줍니다. 그래서 다양한 내장함수를 쓸 수는 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex. 회의록에서 내 이름이 몇번 나왔는지 궁금할 때~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'Python', 'Python']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt3 = \"Python, Python, Python ! it is the best language in the world\"\n",
    "re.findall(\"Python\", txt3) #파이썬이 몇번 나오는지 리스트에 담아서 리스트 리턴해 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 메타문자와 정규표현식\n",
    "\n",
    "위의 $ ^ [ ] | 등을 메타문자라고 부릅니다.\n",
    "\n",
    "이러한 메타문자는 특별한 의미를 가지는 문자입니다.\n",
    "\n",
    "정규표현식을 보다 쉽게 표현할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ". 은 한 문자를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 4), match='ABBA'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search('A..A', 'ABA')) #A 와 A 사이에 .. 문자가 2개 있어야 반환됨 .\n",
    "print(re.search('A..A', 'ABBA')) # 문자가 두개 들어가 있으므로 리턴됨\n",
    "print(re.search('A..A', 'ABBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "? 은 0개 혹은 1개 문자를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "None\n",
      "<re.Match object; span=(3, 4), match='A'>\n",
      "<re.Match object; span=(1, 3), match='AB'>\n",
      "<re.Match object; span=(1, 3), match='AB'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search('AB?', 'A')) #A가 있고, B뒤에 ?이 붙었으므로 B가 0개거나 1개면 됨\n",
    "print(re.search('AB?', 'AA'))\n",
    "print(re.search('AB?', 'J-HOP'))\n",
    "print(re.search('AB?', 'X-MAN'))\n",
    "print(re.search('AB?', 'CABBA'))\n",
    "print(re.search('AB?', 'CABBBBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\* 은 최하 0개 이상의 일치하는 문자를 의미합니다. (즉, 한 개도 없어도 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "None\n",
      "<re.Match object; span=(3, 4), match='A'>\n",
      "<re.Match object; span=(1, 4), match='ABB'>\n",
      "<re.Match object; span=(1, 7), match='ABBBBB'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search('AB*', 'A')) #0개 이상이므로 한개도 없어도 되고 그 이상으로 많아도 된다.\n",
    "print(re.search('AB*', 'AA'))\n",
    "print(re.search('AB*', 'J-HOP'))\n",
    "print(re.search('AB*', 'X-MAN'))\n",
    "print(re.search('AB*', 'CABBA'))\n",
    "print(re.search('AB*', 'CABBBBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\+ 은 최하 1개 이상의 일치하는 문자를 의미합니다. (즉, 한 개라도 있어야 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(1, 4), match='ABB'>\n",
      "<re.Match object; span=(1, 7), match='ABBBBB'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search('AB+', 'A')) # B가 한개라도 있어야 한다. 1개 이상으로 매칭하면 반환해줌\n",
    "print(re.search('AB+', 'AA'))\n",
    "print(re.search('AB+', 'J-HOP'))\n",
    "print(re.search('AB+', 'X-MAN'))\n",
    "print(re.search('AB+', 'CABBA'))\n",
    "print(re.search('AB+', 'CABBBBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "예제를 하나 풀어봅시다.\n",
    "\n",
    "아래에서 AI로 시작하는 과목 코드만 떼어와 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI101', 'AI102']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# 멀티라인 텍스트는 세 개의 따옴표를 사용하여 표현한다\n",
    "text=\"\"\"CS101 Python\n",
    "AI101 AIandsociety\n",
    "AI102 PyTorch\n",
    "CS102 TeamProject\n",
    "\"\"\" \n",
    " \n",
    "s = re.findall('AI\\d+', text)  #d는 10진수의 1자리 숫자들 (0~9) 의미\n",
    "#AI로 시작하고 뒤에가 d 데시멀 숫자로 되어있는 문자열을 매칭해서 리스트로 반환해준다.\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 위의 예 말고도 아주 많은 예가 있습니다. 구글에서 regular expression meta chracter를 검색해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### sub 함수는 정규식에서 매칭된 문자를 대체할 수 있습니다. \n",
    "\n",
    "앞에 찾은 문자열을 뒤의 문자열로 치환해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's Melon chart #1 is Brave Girls\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"Today's Melon chart #1 is IU\"\n",
    "print(re.sub('IU', 'Brave Girls', s)) #IU를 찾아서 브걸로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My password is * * * * *\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My password is * * * * *'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'My password is 1 3 5 102 41'\n",
    "print(re.sub('[0-9]+', '*', s)) #숫자를 모두 암호화해서 *로 바꾸고 싶은 경우!\n",
    "re.sub('\\d+', '*', s)     # 숫자만 찾아서 *으로 바꿈 ([0-9]+ == \\d+) + : 1개이상 일치인 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 자연어 처리를 위한 nltk 모듈을 배워봅시다\n",
    "\n",
    "우리는 위에서 조금 더 복잡한 규칙을 가진 텍스트를 다루는 법을 배웠습니다.\n",
    "\n",
    "하지만 일반적인 자연어는 규칙을 찾아내기 매우 어렵고 까다롭습니다.\n",
    "\n",
    "예를 들어서 문장에서 명사만 찾아내고 싶다면 어떻게 해야할까요?\n",
    "\n",
    "이런 때를 위해 python에는 nltk (Natural Language Tool Kit)이라는 패키지가 있습니다.\n",
    "\n",
    "설치가 되어있지 않다면 아래의 명령어로 설치하시면 됩니다.\n",
    "\n",
    "한국어 텍스트를 다루고 싶다면 오늘 배우지는 않지만 konlpy를 같이 설치합니다\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge nltk\n",
    "pip install nltk\n",
    "pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 토큰화\n",
    "\n",
    "일반적으로 수집한 자연어 데이터는 필요에 맞게 전처리된 상태가 아닙니다.\n",
    "그래서 해당 데이터를 사용하고자 하는 용도에 따라 아래의 3단계 일을 보통 진행하게 됩니다.\n",
    "\n",
    "1. 토큰화 (tokenization)\n",
    "2. 정제 (cleaning)\n",
    "3. 정규화 (normalization)\n",
    "\n",
    "먼저주어진 코퍼스(corpus)단위에서 토큰(token)이라는 작은 단위로 나누는 작업을 토큰화라고 합니다. \n",
    "\n",
    "토큰의 단위는 상황에 따라 다르지만, 보통 단어나 단어 몇개가 모인 구를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 : 문장을 쪼개서 작은 단위의 토큰으로 만듦\n",
    "# 정제 : 규칙성 있게 만들어 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "nltk의 토큰화에는 nltk.tokenize를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jhyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software\n",
      "['Big', 'data', 'is', 'a', 'field', 'that', 'treats', 'ways', 'to', 'analyze', ',', 'systematically', 'extract', 'information', 'from', ',', 'or', 'otherwise', 'deal', 'with', 'data', 'sets', 'that', 'are', 'too', 'large', 'or', 'complex', 'to', 'be', 'dealt', 'with', 'by', 'traditional', 'data-processing', 'application', 'software']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#  punkt라는 데이터셋이 필수적으로 필요합니다.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Specify the title of the Wikipedia page\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0] #위키피디아의 페이지를 불러들여서 . 단위로 끊어준 뒤 0을 넣어 첫번째 문장만 사용\n",
    "\n",
    "print(wiki)\n",
    "print(word_tokenize(wiki)) #잘라낸 문장을 토큰화\n",
    "#기본적으로 단어 단위로 쪼개고, , 쉼표 등의 구도 같이 쪼개줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "word_tokenize()는 TreebankWordTokenizer라는 방식으로 토큰화를 해 줍니다.\n",
    "\n",
    "1980년대 월스트리트 저널의 기사로부터 만들어진 방식입니다.\n",
    "\n",
    "이 방식은 스페이스와 구두점(.)을 이용해서 단어를 분해하고, 구두점을 남겨둡니다. \n",
    "\n",
    "그런데 아래와 같은 경우는 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'stop']\n"
     ]
    }
   ],
   "source": [
    "string = \"I can't stop\"\n",
    "print(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', \"'\", 't', 'stop']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer라는 방식도 있습니다\n",
    "# 구두점 단위로 단순히 나누어줍니다\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정제와 정규화\n",
    "\n",
    "토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning)하거나 정규화(normalization)하는 일이 자주 발생합니다.\n",
    "\n",
    "정제와 정규화는 아래의 목적을 가지고 합니다.\n",
    "\n",
    "정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "\n",
    "정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제 : 쓰지 않는 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "정제와 정규화 기법은 아래와 같은 것들이 있습니다.\n",
    "아래의 기법들 이외에도 다양한 기법이 있지만, 분석의 목적에 맞게 적절한 방법을 사용해야만 합니다.\n",
    "\n",
    "* 규칙에 기반한 단어의 통합\n",
    "    * 동일 단어 매칭 (ex: US, USA)\n",
    "    * 확률적인 방법 (embedding)\n",
    "\n",
    "* 대소문자 통합\n",
    "\n",
    "* 불필요한 단어의 제거\n",
    "    * 불용어 사전 사용\n",
    "    * 등장 빈도가 적은 단어 삭제\n",
    "    * 길이가 매우 짧은 단어를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 의 경우에는 짧은 단어는 불필요한 단어인 경우가 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 품사 태깅(Part-of-speech tagging)\n",
    "\n",
    "단순히 공백 등으로 나누는 것 말고, 단어의 품사를 태깅할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 : 명사, 형용사, 부사 등 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jhyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software\n",
      "[('Big', 'NNP'), ('data', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('that', 'WDT'), ('treats', 'VBZ'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'VB'), (',', ','), ('systematically', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN')]\n",
      "[('data', 'NN'), ('field', 'NN'), ('information', 'NN'), ('deal', 'NN'), ('application', 'NN'), ('software', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') #퍼셉트론 태거를 사용\n",
    "\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0]\n",
    "print(wiki)\n",
    "tokens = word_tokenize(wiki)\n",
    "print(pos_tag(tokens)) # 1. 토큰을 만들어서 \n",
    "nouns = [x for x in pos_tag(tokens) if x[1] ==\"NN\"] # 2. 인자로 토큰 넣어줌 (명사만 갖고 싶다면 조건을 명사 NN로 걸어준다)\n",
    "print(nouns) # 품사를 뒤에 태깅해서 리스트로 리턴해준다. NN -> 명사 .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "단순 단어가 아닌 \"명사구\" 등의 뭉치를 찾고 싶다면 정규식을 이용한 chunking를 해 주면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Big/NNP\n",
      "  (NP data/NN)\n",
      "  is/VBZ\n",
      "  (NP a/DT field/NN)\n",
      "  that/WDT\n",
      "  treats/VBZ\n",
      "  ways/NNS\n",
      "  to/TO\n",
      "  analyze/VB\n",
      "  ,/,\n",
      "  systematically/RB\n",
      "  (NP extract/JJ information/NN)\n",
      "  from/IN\n",
      "  ,/,\n",
      "  or/CC\n",
      "  otherwise/RB\n",
      "  (NP deal/NN)\n",
      "  with/IN\n",
      "  data/NNS\n",
      "  sets/NNS\n",
      "  that/WDT\n",
      "  are/VBP\n",
      "  too/RB\n",
      "  large/JJ\n",
      "  or/CC\n",
      "  complex/JJ\n",
      "  to/TO\n",
      "  be/VB\n",
      "  dealt/VBN\n",
      "  with/IN\n",
      "  by/IN\n",
      "  (NP traditional/JJ data-processing/JJ application/NN)\n",
      "  (NP software/NN))\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0]\n",
    "tokens = word_tokenize(wiki) #동일하게 토큰을 만들어주고\n",
    "tagged_tokens = pos_tag(tokens) #각 단어마다 tag 해줌\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" #DT : 한정사 JJ : 형용사 NN : 명사\n",
    "# 즉, 한정사 뒤에 JJ가 0개 혹은 1개 이상 붙고, 그 뒤에 명사가 0개 이상 붙은 문장을 추출\n",
    "# ex. The super great man\n",
    "cp = nltk.RegexpParser(grammar) #grammar를 집어넣고 parser를 사용해서 청크 생성\n",
    "result = cp.parse(tagged_tokens) # tagged_token에서 위에서 정의한 cp 정규식(NP)를 찾아서 result에 저장\n",
    "print(result)\n",
    "#result.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 이번 주 정리\n",
    "### 배운 것들 \n",
    "* python의 string, string모듈\n",
    "* wordcloud 시각화 방법\n",
    "* 그 이후 정규표현식 (Regular Expressions) 을 통해서 텍스트를 다루는 법\n",
    "* python의 자연어 처리기인 nltk를 통해서 텍스트 데이터를 다루는 법\n",
    "\n",
    "### 자연어 처리 패키지는 다른 것들도 있습니다.\n",
    "* 한국어의 경우 konlpy를 많이 사용해왔지만 성능이 높지 않습니다.\n",
    "    * 좋은 사전을 만드는 데에는 돈과 인력이 많이 들어갑니다.\n",
    "    \n",
    "* 최근에 카카오브레인에서 만든 pororo의 성능이 좋습니다. \n",
    "    * Platform Of neuRal mOdels for natuRal language prOcessing"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
